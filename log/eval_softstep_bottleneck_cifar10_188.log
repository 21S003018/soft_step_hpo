softstep_bottleneck_cifar10/188_o1_cifar-10-batches-py
Files already downloaded and verified
Files already downloaded and verified
Epoch~1->train_loss:1.6499, val_loss:1.3277, val_accu:0.5454, time:36.6816
Epoch~2->train_loss:1.0265, val_loss:0.908, val_accu:0.6801, time:33.5101
Epoch~3->train_loss:0.7504, val_loss:0.7499, val_accu:0.7393, time:33.7287
Epoch~4->train_loss:0.6039, val_loss:0.6471, val_accu:0.7804, time:33.7779
Epoch~5->time:33.7345
Epoch~6->train_loss:0.4556, val_loss:0.5618, val_accu:0.8145, time:34.0882
Epoch~7->train_loss:0.4164, val_loss:0.4952, val_accu:0.8338, time:34.2321
Epoch~8->train_loss:0.3853, val_loss:0.4432, val_accu:0.852, time:34.0882
Epoch~9->train_loss:0.3531, val_loss:0.4223, val_accu:0.8558, time:34.0734
Epoch~10->train_loss:0.323, val_loss:0.3968, val_accu:0.8665, time:34.0889
Epoch~11->train_loss:0.3055, val_loss:0.3938, val_accu:0.87, time:34.1608
Epoch~12->time:33.943
Epoch~13->train_loss:0.2723, val_loss:0.3752, val_accu:0.875, time:34.0463
Epoch~14->time:34.03
Epoch~15->time:33.9476
Epoch~16->train_loss:0.2348, val_loss:0.3646, val_accu:0.8805, time:33.9494
Epoch~17->time:34.0141
Epoch~18->train_loss:0.2178, val_loss:0.3644, val_accu:0.8817, time:33.9532
Epoch~19->train_loss:0.2088, val_loss:0.3222, val_accu:0.894, time:34.0359
Epoch~20->time:34.1335
Epoch~21->time:33.9901
Epoch~22->time:34.0209
Epoch~23->time:34.1087
Epoch~24->time:33.9532
Epoch~25->time:33.9993
Epoch~26->time:34.0439
Epoch~27->train_loss:0.1675, val_loss:0.333, val_accu:0.8947, time:34.0868
Epoch~28->train_loss:0.1575, val_loss:0.3277, val_accu:0.899, time:33.9986
Epoch~29->time:34.0951
Epoch~30->time:34.0284
Epoch~31->time:34.0837
Epoch~32->train_loss:0.144, val_loss:0.3359, val_accu:0.8999, time:34.0292
Epoch~33->time:34.2033
Epoch~34->time:33.9629
Epoch~35->time:34.1002
Epoch~36->time:34.0058
Epoch~37->time:34.1253
Epoch~38->train_loss:0.1318, val_loss:0.3209, val_accu:0.9022, time:33.9469
Epoch~39->time:34.0433
Epoch~40->time:33.9891
Epoch~41->train_loss:0.1215, val_loss:0.3077, val_accu:0.9067, time:34.0588
Epoch~42->time:34.107
Epoch~43->time:33.9649
Epoch~44->train_loss:0.1199, val_loss:0.3001, val_accu:0.909, time:33.9871
Epoch~45->time:34.0322
Epoch~46->time:34.1027
Epoch~47->time:33.9891
Epoch~48->time:34.0957
Epoch~49->time:34.0551
Epoch~50->time:34.1012
Epoch~51->time:34.0725
Epoch~52->time:34.0881
Epoch~53->time:34.093
Epoch~54->time:34.1248
Epoch~55->train_loss:0.1006, val_loss:0.3178, val_accu:0.9116, time:34.1097
Epoch~56->time:33.9009
Epoch~57->time:34.0634
Epoch~58->time:34.1047
Epoch~59->time:34.1175
Epoch~60->time:34.1509
Epoch~61->time:34.0757
Epoch~62->time:33.9927
Epoch~63->train_loss:0.0916, val_loss:0.3048, val_accu:0.913, time:34.1392
Epoch~64->time:34.2598
Epoch~65->time:33.9019
Epoch~66->time:34.0708
Epoch~67->time:34.0954
Epoch~68->time:34.0677
Epoch~69->time:34.0761
Epoch~70->train_loss:0.0811, val_loss:0.3103, val_accu:0.9151, time:34.0103
Epoch~71->time:34.1353
Epoch~72->time:34.2592
Epoch~73->time:34.0297
Epoch~74->time:34.1233
Epoch~75->time:34.2523
Epoch~76->time:34.0051
Epoch~77->time:34.0904
Epoch~78->train_loss:0.0685, val_loss:0.2984, val_accu:0.9184, time:34.109
Epoch~79->train_loss:0.0653, val_loss:0.2796, val_accu:0.9241, time:34.0708
